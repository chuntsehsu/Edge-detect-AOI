{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ORVP_basic_ver1.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1QVra9Kj9yNZNc7P8WXIWe-iGe2KXsZKI",
      "authorship_tag": "ABX9TyMlgi6l8C5yNiN10GVXAs9C",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chuntsehsu/Edge-detect-AOI/blob/main/ORVP_baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuos_o2Q3jG7"
      },
      "source": [
        "# 參考資料"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rkB8Vs1vfaB"
      },
      "source": [
        "# jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B24-S6NMJYaR"
      },
      "source": [
        "# 程式代碼"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oxHQrSyJcrq"
      },
      "source": [
        "## 特徵工程"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeb2oHbONgCz"
      },
      "source": [
        "import os\n",
        "import glob\n",
        "from joblib import Parallel, delayed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sc\n",
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "import warnings"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1TsevNVOrIT"
      },
      "source": [
        "# data_dir = '/content/ORVP/'\n",
        "data_dir = 'C:/Users/User/Desktop/ORVP/'\n",
        "train = pd.read_csv(data_dir+'train.csv')\n",
        "test = pd.read_csv(data_dir+'test.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nZ9w1bwKFm9"
      },
      "source": [
        "def calc_wap1(df):\n",
        "    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "def calc_wap2(df):\n",
        "    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "def calc_wap3(df):\n",
        "    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) / (df['bid_size1'] + df['ask_size1'])\n",
        "    return wap\n",
        "def calc_wap4(df):\n",
        "    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) / (df['bid_size2'] + df['ask_size2'])\n",
        "    return wap\n",
        "# 對股價取log後再相減\n",
        "def log_return(list_stock_prices):\n",
        "    return np.log(list_stock_prices).diff() \n",
        "def realized_volatility(series):\n",
        "    return np.sqrt(np.sum(series**2))\n",
        "def pressure_compute(df):\n",
        "    sell_percent_with_mid_price1 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['ask_price1'])\n",
        "    sell_percent_with_mid_price2 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['ask_price2'])\n",
        "    buy_percent_with_mid_price1 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['bid_price1'])\n",
        "    buy_percent_with_mid_price2 = df[\"mid_price1\"]/(df[\"mid_price1\"]-df['bid_price2'])\n",
        "    df['sell_pressure'] = sell_percent_with_mid_price1/sell_percent_with_mid_price1.sum()*df['ask_size1']+ sell_percent_with_mid_price2/sell_percent_with_mid_price2.sum()*df['ask_size2']\n",
        "    df['buy_pressure'] = buy_percent_with_mid_price1/buy_percent_with_mid_price1.sum()*df['bid_size1'] + buy_percent_with_mid_price2/buy_percent_with_mid_price2.sum()*df['bid_size2']\n",
        "    df[\"pressure_ratio\"] = np.log(df['sell_pressure']) - np.log(df['buy_pressure'])\n",
        "def count_unique(series):\n",
        "    return len(np.unique(series))\n",
        "def read_train_test():\n",
        "    \n",
        "    # Create a key to merge with book and trade data\n",
        "    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n",
        "    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n",
        "\n",
        "    print(f'Our training set has {train.shape[0]} rows')\n",
        "    return train, test\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaYdr33LJf4B"
      },
      "source": [
        "def book_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    ##Price \n",
        "    ##(1) Weighted Price\n",
        "    df['wap1'] = calc_wap1(df)\n",
        "    df['wap2'] = calc_wap2(df)\n",
        "    df['wap3'] = calc_wap3(df)\n",
        "    df['wap4'] = calc_wap4(df)\n",
        "\n",
        "    df['log_return1'] = df.groupby('time_id')['wap1'].apply(log_return)\n",
        "    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n",
        "    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n",
        "    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n",
        "\n",
        "    ##(2) middle price \n",
        "    df[\"mid_price1\"] = (df['ask_price1'] + df['bid_price1'])/2\n",
        "\n",
        "    ##(3)price spread \n",
        "    df['price_spread1'] = (df['ask_price1'] - df['bid_price1']) / ((df['ask_price1'] + df['bid_price1']) / 2)\n",
        "    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) / ((df['ask_price2'] + df['bid_price2']) / 2)\n",
        "\n",
        "    ##(4) volume spread \n",
        "    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n",
        "    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n",
        "    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n",
        "\n",
        "    ##(5) imbalance \n",
        "    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n",
        "    df[\"relative_wap_balance\"] = (df['wap1'] - df['wap2'])/( df['wap1'] + df['wap2'])\n",
        "\n",
        "    ##(6) Volume imbalance \n",
        "    df['depth_imbalance1'] = (df['ask_size1'] - df[\"bid_size1\"])/(df['ask_size1'] + df[\"bid_size1\"])\n",
        "    df['depth_imbalance2'] = (df['ask_size2'] - df[\"bid_size2\"])/(df['ask_size2'] + df[\"bid_size2\"])\n",
        "\n",
        "    ##(7) pressure_compute \n",
        "    pressure_compute(df)\n",
        "    \n",
        "    ##(8)volume imbalance \n",
        "    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n",
        "    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n",
        "\n",
        "\n",
        "\n",
        "    create_feature_dict = {\n",
        "        'wap1': [np.sum, np.max, np.min, np.median, np.std],\n",
        "        'wap2': [np.sum, np.max, np.min, np.median, np.std],\n",
        "        'wap3': [np.sum, np.max, np.min, np.median, np.std],\n",
        "        'wap4': [np.sum, np.max, np.min, np.median, np.std],\n",
        "        \n",
        "        'log_return1': [realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "        'log_return2': [realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "        'log_return3': [realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "        'log_return4': [realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "\n",
        "        \"mid_price1\":[np.sum, np.max, np.min, np.median, np.std],\n",
        "        \n",
        "        'price_spread1':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        'price_spread2':[np.sum, np.max, np.min, np.median, np.std],\n",
        "\n",
        "        'bid_spread':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        'ask_spread':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        \"bid_ask_spread\":[np.sum, np.max, np.min, np.median, np.std],\n",
        "\n",
        "        'wap_balance': [np.sum, np.max, np.min, np.median, np.std],\n",
        "        \"relative_wap_balance\":[np.sum, np.max, np.min, np.median, np.std],\n",
        "\n",
        "        'depth_imbalance1':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        'depth_imbalance2':[np.sum, np.max, np.min, np.median, np.std],\n",
        "\n",
        "        'total_volume':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        'volume_imbalance':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        \n",
        "    }\n",
        "    create_feature_dict_time = {\n",
        "        'log_return1': [realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "        'log_return2': [realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "        'log_return3': [realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "        'log_return4': [realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "    }\n",
        "\n",
        "    \n",
        "    \n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(cf_dict, seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(cf_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(create_feature_dict, seconds_in_bucket = 0, add_suffix = False)\n",
        "    #df_feature_60 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 60, add_suffix = True)\n",
        "    #df_feature_120 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 120, add_suffix = True)\n",
        "    #df_feature_180 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 180, add_suffix = True)\n",
        "    df_feature_240 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 240, add_suffix = True)\n",
        "    #df_feature_300 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 300, add_suffix = True)\n",
        "    #df_feature_360 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 360, add_suffix = True)\n",
        "    #df_feature_420 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 420, add_suffix = True)\n",
        "    df_feature_480 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 480, add_suffix = True)\n",
        "    #df_feature_540 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 540, add_suffix = True)\n",
        "\n",
        "    \n",
        "    # Merge all\n",
        "    #df_feature = df_feature.merge(df_feature_60, how = 'left', left_on = 'time_id_', right_on = 'time_id__60')\n",
        "    #df_feature = df_feature.merge(df_feature_120, how = 'left', left_on = 'time_id_', right_on = 'time_id__120')\n",
        "    #df_feature = df_feature.merge(df_feature_180, how = 'left', left_on = 'time_id_', right_on = 'time_id__180')\n",
        "    df_feature = df_feature.merge(df_feature_240, how = 'left', left_on = 'time_id_', right_on = 'time_id__240')\n",
        "    #df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    #df_feature = df_feature.merge(df_feature_360, how = 'left', left_on = 'time_id_', right_on = 'time_id__360')\n",
        "    #df_feature = df_feature.merge(df_feature_420, how = 'left', left_on = 'time_id_', right_on = 'time_id__420')\n",
        "    df_feature = df_feature.merge(df_feature_480, how = 'left', left_on = 'time_id_', right_on = 'time_id__480')\n",
        "    #df_feature = df_feature.merge(df_feature_540, how = 'left', left_on = 'time_id_', right_on = 'time_id__540')\n",
        "\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop([ 'time_id__240','time_id__480'], axis = 1, inplace = True)\n",
        "    \n",
        "    # Create row_id so we can merge\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n",
        "    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3L3bL_ZL0By"
      },
      "source": [
        "# Function to preprocess trade data (for each stock id)\n",
        "def trade_preprocessor(file_path):\n",
        "    df = pd.read_parquet(file_path)\n",
        "    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n",
        "    df['amount']=df['price']*df['size']\n",
        "    # Dict for aggregations\n",
        "    create_feature_dict = {\n",
        "        'log_return':[realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        'order_count':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        'amount':[np.sum, np.max, np.min, np.median, np.std],\n",
        "    }\n",
        "    create_feature_dict_time = {\n",
        "        'log_return':[realized_volatility, np.sum, np.max, np.min, np.median, np.std],\n",
        "        'seconds_in_bucket':[count_unique],\n",
        "        'size':[np.sum, np.max, np.min, np.median, np.std],\n",
        "        'order_count':[np.sum, np.max, np.min, np.median, np.std],\n",
        "    }\n",
        "    # Function to get group stats for different windows (seconds in bucket)\n",
        "    def get_stats_window(cf_dict, seconds_in_bucket, add_suffix = False):\n",
        "        # Group by the window\n",
        "        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(cf_dict).reset_index()\n",
        "        # Rename columns joining suffix\n",
        "        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n",
        "        # Add a suffix to differentiate windows\n",
        "        if add_suffix:\n",
        "            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n",
        "        return df_feature\n",
        "    \n",
        "    # Get the stats for different windows\n",
        "    df_feature = get_stats_window(create_feature_dict, seconds_in_bucket = 0, add_suffix = False)\n",
        "    #df_feature_60 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 60, add_suffix = True)\n",
        "    #df_feature_120 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 120, add_suffix = True)\n",
        "    #df_feature_180 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 180, add_suffix = True)\n",
        "    df_feature_240 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 240, add_suffix = True)\n",
        "    #df_feature_300 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 300, add_suffix = True)\n",
        "    #df_feature_360 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 360, add_suffix = True)\n",
        "    #df_feature_420 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 420, add_suffix = True)\n",
        "    df_feature_480 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 480, add_suffix = True)\n",
        "    #df_feature_540 = get_stats_window(create_feature_dict_time, seconds_in_bucket = 540, add_suffix = True)\n",
        "\n",
        "    def tendency(price, vol):    \n",
        "        df_diff = np.diff(price)\n",
        "        val = (df_diff/price[1:])*100\n",
        "        power = np.sum(val*vol[1:])\n",
        "        return(power)\n",
        "    \n",
        "    lis = []\n",
        "    for n_time_id in df['time_id'].unique():\n",
        "        df_id = df[df['time_id'] == n_time_id]        \n",
        "        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n",
        "        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n",
        "        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n",
        "        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n",
        "        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n",
        "\n",
        "        # PoV(Percent of Volume)\n",
        "        # new\n",
        "        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n",
        "        energy = np.mean(df_id['price'].values**2)\n",
        "        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n",
        "        \n",
        "        # vol vars    \n",
        "        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n",
        "        energy_v = np.sum(df_id['size'].values**2)\n",
        "        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n",
        "        \n",
        "        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n",
        "                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n",
        "    \n",
        "    df_lr = pd.DataFrame(lis)\n",
        "        \n",
        "   \n",
        "    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n",
        "    \n",
        "    # Merge all\n",
        "    #df_feature = df_feature.merge(df_feature_60, how = 'left', left_on = 'time_id_', right_on = 'time_id__60')\n",
        "    #df_feature = df_feature.merge(df_feature_120, how = 'left', left_on = 'time_id_', right_on = 'time_id__120')\n",
        "    #df_feature = df_feature.merge(df_feature_180, how = 'left', left_on = 'time_id_', right_on = 'time_id__180')\n",
        "    df_feature = df_feature.merge(df_feature_240, how = 'left', left_on = 'time_id_', right_on = 'time_id__240')\n",
        "    #df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n",
        "    #df_feature = df_feature.merge(df_feature_360, how = 'left', left_on = 'time_id_', right_on = 'time_id__360')\n",
        "    #df_feature = df_feature.merge(df_feature_420, how = 'left', left_on = 'time_id_', right_on = 'time_id__420')\n",
        "    df_feature = df_feature.merge(df_feature_480, how = 'left', left_on = 'time_id_', right_on = 'time_id__480')\n",
        "    #df_feature = df_feature.merge(df_feature_540, how = 'left', left_on = 'time_id_', right_on = 'time_id__540')\n",
        "\n",
        "    # Drop unnecesary time_ids\n",
        "    df_feature.drop(['time_id__240','time_id__480'], axis = 1, inplace = True)\n",
        "\n",
        "    \n",
        "    \n",
        "    df_feature = df_feature.add_prefix('trade_')\n",
        "    stock_id = file_path.split('=')[1]\n",
        "    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n",
        "    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n",
        "    return df_feature"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlRwF2F3MArJ"
      },
      "source": [
        "# Function to get group stats for the stock_id and time_id\n",
        "def get_time_stock(df):\n",
        "    # Get realized volatility columns\n",
        "    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', \n",
        "            #'log_return1_realized_volatility_60', 'log_return2_realized_volatility_60', \n",
        "            #'log_return1_realized_volatility_120', 'log_return2_realized_volatility_120', \n",
        "            #'log_return1_realized_volatility_180', 'log_return2_realized_volatility_180', \n",
        "            'log_return1_realized_volatility_240', 'log_return2_realized_volatility_240', \n",
        "            #'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', \n",
        "            #'log_return1_realized_volatility_360', 'log_return2_realized_volatility_360',\n",
        "            #'log_return1_realized_volatility_420', 'log_return2_realized_volatility_420', \n",
        "            'log_return1_realized_volatility_480', 'log_return2_realized_volatility_480', \n",
        "            #'log_return1_realized_volatility_540', 'log_return2_realized_volatility_540',\n",
        "            'trade_log_return_realized_volatility', #'trade_log_return_realized_volatility_60', \n",
        "            #'trade_log_return_realized_volatility_120', #'trade_log_return_realized_volatility_180',\n",
        "            'trade_log_return_realized_volatility_240', #'trade_log_return_realized_volatility_300', \n",
        "            #'trade_log_return_realized_volatility_360', #'trade_log_return_realized_volatility_420',\n",
        "            'trade_log_return_realized_volatility_480', #'trade_log_return_realized_volatility_540'\n",
        "            ]\n",
        "    \n",
        "\n",
        "    # Group by the stock id\n",
        "    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', 'median' ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n",
        "    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n",
        "\n",
        "    # Group by the stock id\n",
        "    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', 'median' ]).reset_index()\n",
        "    # Rename columns joining suffix\n",
        "    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n",
        "    df_time_id = df_time_id.add_suffix('_' + 'time')\n",
        "    \n",
        "    # Merge with original dataframe\n",
        "    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n",
        "    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n",
        "    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n",
        "    return df"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbfnxp9oMzHD"
      },
      "source": [
        "# Funtion to make preprocessing function in parallel (for each stock id)\n",
        "def preprocessor(list_stock_ids, is_train = True):\n",
        "    \n",
        "    # Parrallel for loop\n",
        "    def for_joblib(stock_id):\n",
        "        # Train\n",
        "        if is_train:\n",
        "            file_path_book = data_dir + \"book_train.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_train.parquet/stock_id=\" + str(stock_id)\n",
        "        # Test\n",
        "        else:\n",
        "            file_path_book = data_dir + \"book_test.parquet/stock_id=\" + str(stock_id)\n",
        "            file_path_trade = data_dir + \"trade_test.parquet/stock_id=\" + str(stock_id)\n",
        "    \n",
        "        # Preprocess book and trade data and merge them\n",
        "        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n",
        "        \n",
        "        # Return the merge dataframe\n",
        "        return df_tmp\n",
        "    \n",
        "    # Use parallel api to call paralle for loop\n",
        "    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n",
        "    # Concatenate all the dataframes that return from Parallel\n",
        "    df = pd.concat(df, ignore_index = True)\n",
        "    return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5V4A-GlM0qy"
      },
      "source": [
        "# Function to calculate the root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRb6GX31Fx27",
        "outputId": "70bffd8b-d95d-4b5c-8b8b-28d0c1592a5d"
      },
      "source": [
        "# Read train and test\n",
        "train, test = read_train_test()\n",
        "\n",
        "# Get unique stock ids \n",
        "train_stock_ids = train['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "train_ = preprocessor(train_stock_ids, is_train = True)\n",
        "train = train.merge(train_, on = ['row_id'], how = 'left')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our training set has 428932 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   53.9s\n",
            "C:\\commfile\\Anaconda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44gHJp0IF4Oc"
      },
      "source": [
        "corr_matrix = train.corr()\n",
        "train_corr = corr_matrix['target'].sort_values(ascending = False)\n",
        "train_corr.to_csv(data_dir+'train_corr.csv',index=True)\n",
        "train_list = train_corr[abs(train_corr.values) > 0.3].index\n",
        "n_train = pd.concat([train.iloc[:,:2],train.loc[:,train_list]],axis=1)\n",
        "n_train.to_csv(data_dir+'import.csv',index=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVq29QJTNK9U",
        "outputId": "2d8165d8-2592-4e12-ee17-750f3842e02e"
      },
      "source": [
        "\n",
        "\n",
        "# Get unique stock ids \n",
        "test_stock_ids = test['stock_id'].unique()\n",
        "# Preprocess them using Parallel and our single stock id functions\n",
        "test_ = preprocessor(test_stock_ids, is_train = False)\n",
        "test = test.merge(test_, on = ['row_id'], how = 'left')\n",
        "\n",
        "# Get group stats of time_id and stock_id\n",
        "train = get_time_stock(train)\n",
        "test = get_time_stock(test)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our training set has 428932 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:   53.4s\n",
            "C:\\commfile\\Anaconda\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[Parallel(n_jobs=-1)]: Done 112 out of 112 | elapsed:  3.2min finished\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:    0.0s finished\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1ZYd_w3XIl"
      },
      "source": [
        "# replace by order sum (tau)\n",
        "train['size_tau'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique'] )\n",
        "test['size_tau'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique'] )\n",
        "# train['size_tau_120'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_120'] )\n",
        "# test['size_tau_120'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_120'] )\n",
        "train['size_tau_240'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_240'] )\n",
        "test['size_tau_240'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_240'] )\n",
        "# train['size_tau_360'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_360'] )\n",
        "# test['size_tau_360'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_360'] )\n",
        "train['size_tau_480'] = np.sqrt( 1/ train['trade_seconds_in_bucket_count_unique_480'] )\n",
        "test['size_tau_480'] = np.sqrt( 1/ test['trade_seconds_in_bucket_count_unique_480'] )"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9oNZq0G3cjG"
      },
      "source": [
        "train['size_tau2'] = np.sqrt( 1/ train['trade_order_count_sum'] )\n",
        "test['size_tau2'] = np.sqrt( 1/ test['trade_order_count_sum'] )\n",
        "train['size_tau2_480'] = np.sqrt( 0.33/ train['trade_order_count_sum'] )\n",
        "test['size_tau2_480'] = np.sqrt( 0.33/ test['trade_order_count_sum'] )\n",
        "# train['size_tau2_360'] = np.sqrt( 0.44/ train['trade_order_count_sum'] )\n",
        "# test['size_tau2_360'] = np.sqrt( 0.44/ test['trade_order_count_sum'] )\n",
        "train['size_tau2_240'] = np.sqrt( 0.5/ train['trade_order_count_sum'] )\n",
        "test['size_tau2_240'] = np.sqrt( 0.5/ test['trade_order_count_sum'] )\n",
        "# train['size_tau2_120'] = np.sqrt( 0.66/ train['trade_order_count_sum'] )\n",
        "# test['size_tau2_120'] = np.sqrt( 0.66/ test['trade_order_count_sum'] )\n",
        "\n",
        "# delta tau\n",
        "train['size_tau2_d'] = train['size_tau2_480'] - train['size_tau2']\n",
        "test['size_tau2_d'] = test['size_tau2_480'] - test['size_tau2']"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpnkPsu03xZO",
        "outputId": "493a433d-6643-476a-a62d-9aa9e462262b"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "# making agg features\n",
        "\n",
        "train_p = train.copy()\n",
        "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "corr = train_p.corr()\n",
        "\n",
        "ids = corr.index\n",
        "n_clusters = 8\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=2021).fit(corr.values)\n",
        "print(kmeans.labels_)\n",
        "\n",
        "l = []\n",
        "for n in range(n_clusters):\n",
        "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
        "    \n",
        "\n",
        "mat = []\n",
        "matTest = []\n",
        "\n",
        "n = 0\n",
        "for ind in l:\n",
        "    print(ind)\n",
        "    newDf = train.loc[train['stock_id'].isin(ind) ]\n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    mat.append ( newDf )\n",
        "    \n",
        "    newDf = test.loc[test['stock_id'].isin(ind) ]    \n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    matTest.append ( newDf )\n",
        "    \n",
        "    n+=1\n",
        "    \n",
        "mat1 = pd.concat(mat).reset_index()\n",
        "mat1.drop(columns=['target'],inplace=True)\n",
        "mat2 = pd.concat(matTest).reset_index()\n",
        "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\n",
        "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
        "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
        "mat1.reset_index(inplace=True)\n",
        "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
        "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
        "mat2.reset_index(inplace=True)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 4 5 6 1 1 6 5 3 6 4 4 5 5 1 1 1 6 5 5 7 4 1 1 7 1 1 5 7 5 7 5 5 1 7 7 5\n",
            " 7 5 1 5 1 5 5 1 0 5 5 5 0 4 7 7 7 6 0 6 5 1 5 5 1 5 1 4 7 0 4 7 0 3 2 7 0\n",
            " 0 1 0 4 7 0 0 5 1 1 0 0 7 7 1 0 1 5 5 5 5 1 1 7 1 4 1 5 1 4 1 5 1 4 5 0 5\n",
            " 4]\n",
            "[50, 55, 62, 75, 78, 83, 84, 86, 89, 90, 96, 97, 101, 124]\n",
            "[0, 4, 5, 15, 16, 17, 23, 26, 28, 29, 36, 42, 44, 48, 66, 69, 72, 85, 94, 95, 100, 102, 108, 109, 111, 113, 115, 118, 120]\n",
            "[81]\n",
            "[8, 80]\n",
            "[1, 10, 11, 22, 56, 73, 76, 87, 112, 116, 122, 126]\n",
            "[2, 7, 13, 14, 19, 20, 30, 32, 34, 35, 39, 41, 43, 46, 47, 51, 52, 53, 64, 67, 68, 70, 93, 103, 104, 105, 107, 114, 119, 123, 125]\n",
            "[3, 6, 9, 18, 61, 63]\n",
            "[21, 27, 31, 33, 37, 38, 40, 58, 59, 60, 74, 77, 82, 88, 98, 99, 110]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OncDGJY32QN"
      },
      "source": [
        "train = pd.merge(train,mat1,how='left',on='time_id')\n",
        "test = pd.merge(test,mat2,how='left',on='time_id')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbv1o_DJJz2J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "io8lwglO36ee"
      },
      "source": [
        "import gc\n",
        "del mat1,mat2\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCBQUPLyEhzW"
      },
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import lightgbm as lgb\n",
        "\n",
        "fold_n = 8\n",
        "\n",
        "seed0=2021\n",
        "params0 = {\n",
        "    'objective': 'rmse',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'max_depth': -1,\n",
        "    'max_bin':100,\n",
        "    'min_data_in_leaf':500,\n",
        "    'learning_rate': 0.2,\n",
        "    'subsample': 0.72,\n",
        "    'subsample_freq': 4,\n",
        "    'feature_fraction': 0.5,\n",
        "    'lambda_l1': 0.5,\n",
        "    'lambda_l2': 1.0,\n",
        "    'categorical_column':[0],\n",
        "    'seed':seed0,\n",
        "    'feature_fraction_seed': seed0,\n",
        "    'bagging_seed': seed0,\n",
        "    'drop_seed': seed0,\n",
        "    'data_random_seed': seed0,\n",
        "    'n_jobs':-1,\n",
        "    'verbose': -1}\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False\n",
        "# learning rate schedule\n",
        "def learning_rate_decay(current_iter):\n",
        "  base_learning_rate = params0.get('learning_rate') \n",
        "  \n",
        "  lr = base_learning_rate * np.power(.998, current_iter) + base_learning_rate ** (current_iter/1000)\n",
        "  if current_iter % 50 == 0:\n",
        "    print(f'---------- learning_rate : {lr} ----------') \n",
        "  \n",
        "  return lr \n",
        "def train_and_evaluate_lgb(train, test, params):\n",
        "    # Hyperparammeters (just basic)\n",
        "    \n",
        "    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n",
        "    y = train['target']\n",
        "    # Create out of folds array\n",
        "    oof_predictions = np.zeros(train.shape[0])\n",
        "    # Create test array to store predictions\n",
        "    test_predictions = np.zeros(test.shape[0])\n",
        "    # Create a KFold object\n",
        "    kfold = KFold(n_splits = fold_n, random_state = 2021, shuffle = True)\n",
        "    # Iterate through each fold\n",
        "    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n",
        "        print(f'Training fold {fold + 1}')\n",
        "        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n",
        "        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "        # Root mean squared percentage error weights\n",
        "        train_weights = 1 / np.square(y_train)\n",
        "        val_weights = 1 / np.square(y_val)\n",
        "        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n",
        "        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n",
        "        model = lgb.train(params = params,\n",
        "                          num_boost_round=1501,\n",
        "                          train_set = train_dataset, \n",
        "                          valid_sets = [train_dataset, val_dataset], \n",
        "                          verbose_eval = 250,\n",
        "                          early_stopping_rounds=50,\n",
        "                          feval = feval_rmspe,\n",
        "                          callbacks=[lgb.reset_parameter(learning_rate=learning_rate_decay)])\n",
        "        # Add predictions to the out of folds array\n",
        "        oof_predictions[val_ind] = model.predict(x_val[features])\n",
        "        # Predict the test set\n",
        "        test_predictions += model.predict(test[features]) / fold_n\n",
        "\n",
        "        lgb.plot_importance(model,max_num_features=20)\n",
        "        \n",
        "\n",
        "    rmspe_score = rmspe(y, oof_predictions)\n",
        "    print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "    lgb.plot_importance(model,max_num_features=20)\n",
        "\n",
        "    # Return test predictions\n",
        "    return test_predictions\n",
        "# Traing and evaluate\n",
        "predictions_lgb= train_and_evaluate_lgb(n_train, test,params0)\n",
        "test['target'] = predictions_lgb\n",
        "test[['row_id', 'target']].to_csv(data_dir+'submission.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz5QCWFVNBcs"
      },
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# def train_and_evaluate(train, test):\n",
        "#     # Hyperparammeters (optimized)\n",
        "#     seed = 42\n",
        "#     params = {\n",
        "#         'learning_rate': 0.22,        \n",
        "#         'lambda_l1': 2,\n",
        "#         'lambda_l2': 7,\n",
        "#         'num_leaves': 800,\n",
        "#         'min_sum_hessian_in_leaf': 20,\n",
        "#         'feature_fraction': 0.8,\n",
        "#         'feature_fraction_bynode': 0.8,\n",
        "#         'bagging_fraction': 0.9,\n",
        "#         'bagging_freq': 42,\n",
        "#         'min_data_in_leaf': 700,\n",
        "#         'max_depth': 4,\n",
        "#         'seed': seed,\n",
        "#         'feature_fraction_seed': seed,\n",
        "#         'bagging_seed': seed,\n",
        "#         'drop_seed': seed,\n",
        "#         'data_random_seed': seed,\n",
        "#         'objective': 'rmse',\n",
        "#         'boosting': 'gbdt',\n",
        "#         'verbosity': -1,\n",
        "#         'n_jobs': -1,\n",
        "#     }   \n",
        "#     # learning rate schedule\n",
        "#     def learning_rate_decay(current_iter):\n",
        "#       base_learning_rate = params.get('learning_rate')\n",
        "      \n",
        "#       lr = base_learning_rate * np.power(.995, current_iter) + 0.08\n",
        "#       if current_iter % 50 == 0:\n",
        "#         print(f'---------- learning_rate : {lr} ----------') \n",
        "      \n",
        "#       return lr \n",
        "\n",
        "#     # Split features and target\n",
        "#     x = train.drop(['row_id', 'target', 'time_id'], axis = 1)\n",
        "#     y = train['target']\n",
        "#     x_test = test.drop(['row_id', 'time_id'], axis = 1)\n",
        "#     # Transform stock id to a numeric value\n",
        "#     x['stock_id'] = x['stock_id'].astype(int)\n",
        "#     x_test['stock_id'] = x_test['stock_id'].astype(int)\n",
        "    \n",
        "#     # Create out of folds array\n",
        "#     oof_predictions = np.zeros(x.shape[0])\n",
        "#     # Create test array to store predictions\n",
        "#     test_predictions = np.zeros(x_test.shape[0])\n",
        "#     # Create a KFold object\n",
        "#     kfold = KFold(n_splits = 4, random_state = seed, shuffle = True)\n",
        "#     # Iterate through each fold\n",
        "#     for fold, (trn_ind, val_ind) in enumerate(kfold.split(x)):\n",
        "#         print(f'Training fold {fold + 1}')\n",
        "#         x_train, x_val = x.iloc[trn_ind], x.iloc[val_ind]\n",
        "#         y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n",
        "#         # Root mean squared percentage error weights\n",
        "#         train_weights = 1 / np.square(y_train)\n",
        "#         val_weights = 1 / np.square(y_val)\n",
        "#         train_dataset = lgb.Dataset(x_train, y_train, weight = train_weights, categorical_feature = ['stock_id'])\n",
        "#         val_dataset = lgb.Dataset(x_val, y_val, weight = val_weights, categorical_feature = ['stock_id'])\n",
        "#         model = lgb.train(params = params, \n",
        "#                           train_set = train_dataset, \n",
        "#                           valid_sets = [train_dataset, val_dataset], \n",
        "#                           num_boost_round = 3000, \n",
        "#                           early_stopping_rounds = 50, \n",
        "#                           verbose_eval = 100,\n",
        "#                           feval = feval_rmspe,\n",
        "#                           callbacks=[lgb.reset_parameter(learning_rate=learning_rate_decay)])\n",
        "\n",
        "#         plt.figure(figsize=(12,6))\n",
        "#         lgb.plot_importance(model, max_num_features=20)\n",
        "#         plt.title(\"Feature importance\")\n",
        "#         plt.show()\n",
        "#         # Add predictions to the out of folds array\n",
        "#         oof_predictions[val_ind] = model.predict(x_val)\n",
        "#         # Predict the test set\n",
        "#         test_predictions += model.predict(x_test) / 4\n",
        "        \n",
        "#     rmspe_score = rmspe(y, oof_predictions)\n",
        "#     print(f'Our out of folds RMSPE is {rmspe_score}')\n",
        "#     # Return test predictions\n",
        "#     return test_predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R9Rg2dzNNcj"
      },
      "source": [
        "# # Traing and evaluate\n",
        "# test_predictions = train_and_evaluate(train, test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzM7IBWOQUxF"
      },
      "source": [
        " kaggle (baseline.ver1 learning rate 0.212) = Our out of folds RMSPE is 0.19834292014995092  \n",
        " kaggle (baseline_lgb_ver2 learning rate 0.211) = Our out of folds RMSPE is 0.19838470092173055  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxUG_OfJUKMq"
      },
      "source": [
        "# # Save test predictions\n",
        "# test['target'] = test_predictions\n",
        "# test[['row_id', 'target']].to_csv('submission.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1_rciFked3A"
      },
      "source": [
        "# FFNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJUKQevVehLY"
      },
      "source": [
        "from numpy.random import seed\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "import numpy.matlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmOCHYqLejaQ"
      },
      "source": [
        "def root_mean_squared_per_error(y_true, y_pred):\n",
        "  return K.sqrt(K.mean(K.square( (y_true - y_pred)/ y_true )))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8QigkcE4RMP"
      },
      "source": [
        "out_train = pd.read_csv(data_dir+'train.csv')\n",
        "out_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "#out_train[out_train.isna().any(axis=1)]\n",
        "out_train = out_train.fillna(out_train.mean())\n",
        "out_train.head()\n",
        "\n",
        "# code to add the just the read data after first execution\n",
        "\n",
        "# data separation based on knn ++\n",
        "nfolds = 8 # number of folds\n",
        "index = []\n",
        "totDist = []\n",
        "values = []\n",
        "# generates a matriz with the values of \n",
        "mat = out_train.values\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "mat = scaler.fit_transform(mat)\n",
        "\n",
        "nind = int(mat.shape[0]/nfolds) # number of individuals\n",
        "\n",
        "# adds index in the last column\n",
        "mat = np.c_[mat,np.arange(mat.shape[0])]\n",
        "\n",
        "\n",
        "lineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\n",
        "\n",
        "lineNumber = np.sort(lineNumber)[::-1]\n",
        "\n",
        "for n in range(nfolds):\n",
        "    totDist.append(np.zeros(mat.shape[0]-nfolds))\n",
        "\n",
        "# saves index\n",
        "for n in range(nfolds):\n",
        "    \n",
        "    values.append([lineNumber[n]])    \n",
        "\n",
        "\n",
        "s=[]\n",
        "for n in range(nfolds):\n",
        "    s.append(mat[lineNumber[n],:])\n",
        "    \n",
        "    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n",
        "\n",
        "for n in range(nind-1):    \n",
        "\n",
        "    luck = np.random.uniform(0,1,nfolds)\n",
        "    \n",
        "    for cycle in range(nfolds):\n",
        "         # saves the values of index           \n",
        "\n",
        "        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n",
        "\n",
        "        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n",
        "        totDist[cycle] += sumDist        \n",
        "                \n",
        "        # probabilities\n",
        "        f = totDist[cycle]/np.sum(totDist[cycle]) # normalizing the totdist\n",
        "        j = 0\n",
        "        kn = 0\n",
        "        for val in f:\n",
        "            j += val        \n",
        "            if (j > luck[cycle]): # the column was selected\n",
        "                break\n",
        "            kn +=1\n",
        "        lineNumber[cycle] = kn\n",
        "        \n",
        "        # delete line of the value added    \n",
        "        for n_iter in range(nfolds):\n",
        "            \n",
        "            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n",
        "            j= 0\n",
        "        \n",
        "        s[cycle] = mat[lineNumber[cycle],:]\n",
        "        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n",
        "        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n",
        "\n",
        "\n",
        "for n_mod in range(nfolds):\n",
        "    values[n_mod] = out_train.index[values[n_mod]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkQ7OntsMCk3"
      },
      "source": [
        "colNames = [col for col in list(train.columns)\n",
        "            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\n",
        "len(colNames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S-WZytDzMVb"
      },
      "source": [
        "colNames"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FboVBcl4Wqv"
      },
      "source": [
        "from sklearn.preprocessing import QuantileTransformer\n",
        "#colNames.remove('row_id')\n",
        "train.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "test.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
        "qt_train = []\n",
        "train_nn=train[colNames].copy()\n",
        "test_nn=test[colNames].copy()\n",
        "for col in colNames:\n",
        "    #print(col)\n",
        "    qt = QuantileTransformer(random_state=2021,n_quantiles=2000, output_distribution='normal')\n",
        "    train_nn[col] = qt.fit_transform(train_nn[[col]])\n",
        "    test_nn[col] = qt.transform(test_nn[[col]])    \n",
        "    qt_train.append(qt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjY8Qu7Jw2Oh"
      },
      "source": [
        "qt_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIQzt_654YGA"
      },
      "source": [
        "train_nn[['stock_id','time_id','target']]=train[['stock_id','time_id','target']]\n",
        "test_nn[['stock_id','time_id']]=test[['stock_id','time_id']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OC0klh6O4aC_"
      },
      "source": [
        "# making agg features\n",
        "from sklearn.cluster import KMeans\n",
        "train_p = pd.read_csv(data_dir+'train.csv')\n",
        "train_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n",
        "\n",
        "corr = train_p.corr()\n",
        "\n",
        "ids = corr.index\n",
        "\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=2021).fit(corr.values)\n",
        "print(kmeans.labels_)\n",
        "\n",
        "l = []\n",
        "for n in range(n_clusters):\n",
        "    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n",
        "    \n",
        "\n",
        "mat = []\n",
        "matTest = []\n",
        "\n",
        "n = 0\n",
        "for ind in l:\n",
        "    print(ind)\n",
        "    newDf = train_nn.loc[train_nn['stock_id'].isin(ind) ]\n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    mat.append ( newDf )\n",
        "    \n",
        "    newDf = test_nn.loc[test_nn['stock_id'].isin(ind) ]    \n",
        "    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n",
        "    newDf.loc[:,'stock_id'] = str(n)+'c1'\n",
        "    matTest.append ( newDf )\n",
        "    \n",
        "    n+=1\n",
        "    \n",
        "mat1 = pd.concat(mat).reset_index()\n",
        "mat1.drop(columns=['target'],inplace=True)\n",
        "\n",
        "mat2 = pd.concat(matTest).reset_index()\n",
        "mat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdTfXf6H4dNe"
      },
      "source": [
        "mat1 = mat1.pivot(index='time_id', columns='stock_id')\n",
        "mat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\n",
        "mat1.reset_index(inplace=True)\n",
        "\n",
        "mat2 = mat2.pivot(index='time_id', columns='stock_id')\n",
        "mat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\n",
        "mat2.reset_index(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6UaBGsy4eKT"
      },
      "source": [
        "import gc\n",
        "train_nn = pd.merge(train_nn,mat1[nnn],how='left',on='time_id')\n",
        "test_nn = pd.merge(test_nn,mat2[nnn],how='left',on='time_id')\n",
        "# train_nn = pd.merge(train_nn,mat1,how='left',on='time_id')\n",
        "# test_nn = pd.merge(test_nn,mat2,how='left',on='time_id')\n",
        "# del mat1,mat2\n",
        "# del train,test\n",
        "# gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6vMFAQQFndk"
      },
      "source": [
        "train_nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ6LFUSBhL_n"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut2WmKXA4gjf"
      },
      "source": [
        "from keras.backend import sigmoid\n",
        "def swish(x, beta = 1):\n",
        "    return (x * sigmoid(beta * x))\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from keras.layers import Activation\n",
        "get_custom_objects().update({'swish': Activation(swish)})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def base_model():\n",
        "    \n",
        "    hidden_units = (128,64,32)\n",
        "    stock_embedding_size = 24\n",
        "    cat_data = train_nn['stock_id']\n",
        "\n",
        "    # Each instance will consist of two inputs: a single user id, and a single movie id\n",
        "    stock_id_input = keras.Input(shape=(1,), name='stock_id')\n",
        "    num_input = keras.Input(shape=(884,), name='num_data')\n",
        "\n",
        "\n",
        "    #embedding, flatenning and concatenating\n",
        "    stock_embedded = keras.layers.Embedding(max(cat_data)+1, stock_embedding_size, \n",
        "                                           input_length=1, name='stock_embedding')(stock_id_input)\n",
        "    stock_flattened = keras.layers.Flatten()(stock_embedded)\n",
        "    out = keras.layers.Concatenate()([stock_flattened, num_input])\n",
        "    \n",
        "    # Add one or more hidden layers\n",
        "    for n_hidden in hidden_units:\n",
        "\n",
        "        out = keras.layers.Dense(n_hidden, activation='swish')(out)\n",
        "        \n",
        "\n",
        "    #out = keras.layers.Concatenate()([out, num_input])\n",
        "\n",
        "    # A single output: our predicted rating\n",
        "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
        "    \n",
        "    model = keras.Model(\n",
        "    inputs = [stock_id_input, num_input],\n",
        "    outputs = out,\n",
        "    )\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2xjOQDJ4huP"
      },
      "source": [
        "def rmspe(y_true, y_pred):\n",
        "    return np.sqrt(np.mean(np.square((y_true - y_pred) / y_true)))\n",
        "\n",
        "# Function to early stop with root mean squared percentage error\n",
        "def feval_rmspe(y_pred, lgb_train):\n",
        "    y_true = lgb_train.get_label()\n",
        "    return 'RMSPE', rmspe(y_true, y_pred), False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6HRSa20hh50"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVAsYCjC4kEO"
      },
      "source": [
        "from sklearn import preprocessing, model_selection\n",
        "target_name='target'\n",
        "scores_folds = {}\n",
        "model_name = 'NN'\n",
        "pred_name = 'pred_{}'.format(model_name)\n",
        "\n",
        "n_folds = 8\n",
        "kf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2021)\n",
        "scores_folds[model_name] = []\n",
        "counter = 1\n",
        "\n",
        "features_to_consider = list(train_nn)\n",
        "features_to_consider.remove('time_id')\n",
        "features_to_consider.remove('target')\n",
        "# features_to_consider.remove('row_id')\n",
        "try:\n",
        "    features_to_consider.remove('pred_NN')\n",
        "except:\n",
        "    pass\n",
        "\n",
        "\n",
        "train_nn[features_to_consider] = train_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
        "test_nn[features_to_consider] = test_nn[features_to_consider].fillna(train_nn[features_to_consider].mean())\n",
        "\n",
        "train_nn[pred_name] = 0\n",
        "test_nn[target_name] = 0\n",
        "test_predictions_nn = np.zeros(test_nn.shape[0])\n",
        "\n",
        "for n_count in range(n_folds):\n",
        "    print('CV {}/{}'.format(counter, n_folds))\n",
        "    \n",
        "    indexes = np.arange(nfolds).astype(int)    \n",
        "    indexes = np.delete(indexes,obj=n_count, axis=0) \n",
        "    \n",
        "    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n",
        "    \n",
        "    X_train = train_nn.loc[train_nn.time_id.isin(indexes), features_to_consider]\n",
        "    y_train = train_nn.loc[train_nn.time_id.isin(indexes), target_name]\n",
        "    X_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), features_to_consider]\n",
        "    y_test = train_nn.loc[train_nn.time_id.isin(values[n_count]), target_name]\n",
        "    \n",
        "    # NN   \n",
        "    model = base_model()\n",
        "    es = tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=50, verbose=0,\n",
        "            mode='min',restore_best_weights=True)\n",
        "\n",
        "    plateau = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor='val_loss', factor=0.2, patience=7, verbose=1,\n",
        "            mode='min')# kfold based on the knn++ algorithm\n",
        "    model.compile(\n",
        "        keras.optimizers.Adam(learning_rate=0.006),\n",
        "        loss=root_mean_squared_per_error\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        features_to_consider.remove('stock_id')\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    num_data = X_train[features_to_consider]\n",
        "    \n",
        "    scaler = MinMaxScaler(feature_range=(-1, 1))         \n",
        "    num_data = scaler.fit_transform(num_data.values)    \n",
        "    \n",
        "    cat_data = X_train['stock_id']    \n",
        "    target =  y_train\n",
        "    \n",
        "    num_data_test = X_test[features_to_consider]\n",
        "    num_data_test = scaler.transform(num_data_test.values)\n",
        "    cat_data_test = X_test['stock_id']\n",
        "\n",
        "    model.fit([cat_data, num_data], \n",
        "              target,               \n",
        "              batch_size=2048,\n",
        "              epochs=1280,\n",
        "              validation_data=([cat_data_test, num_data_test], y_test),\n",
        "              callbacks=[es, plateau],\n",
        "              validation_batch_size=len(y_test),\n",
        "              shuffle=True,\n",
        "              verbose = 1)\n",
        "\n",
        "    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n",
        "    \n",
        "    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n",
        "    print('Fold {} {}: {}'.format(counter, model_name, score))\n",
        "    scores_folds[model_name].append(score)\n",
        "    \n",
        "    tt =scaler.transform(test_nn[features_to_consider].values)\n",
        "    #test_nn[target_name] += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n",
        "    test_predictions_nn += model.predict([test_nn['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)/n_folds\n",
        "    #test[target_name] += model.predict([test['stock_id'], test[features_to_consider]]).reshape(1,-1)[0].clip(0,1e10)\n",
        "       \n",
        "    counter += 1\n",
        "    features_to_consider.append('stock_id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6Hfrdwz4pOP"
      },
      "source": [
        "test_nn[\"row_id\"] = test_nn[\"stock_id\"].astype(str) + \"-\" + test_nn[\"time_id\"].astype(str) \n",
        "test_nn[target_name] = (test_predictions_nn+predictions_lgb)/2\n",
        "\n",
        "score = round(rmspe(y_true = train_nn[target_name].values, y_pred = train_nn[pred_name].values),5)\n",
        "print('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n",
        "\n",
        "display(test_nn[['row_id', target_name]].head(3))\n",
        "test_nn[['row_id', target_name]].to_csv('submission2.csv',index = False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}